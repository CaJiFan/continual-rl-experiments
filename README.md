# Continual Reinforcement Learning Experiments

This repository contains a collection of **toy experiments and implementations in Continual Reinforcement Learning (CRL)**.  
The goal is to explore methods that enable RL agents to learn across multiple tasks sequentially while mitigating **catastrophic forgetting** and improving **generalization**.

---

## ðŸŽ¯ Motivation
Traditional RL assumes training on a single task/environment until convergence.  
However, real-world agents (e.g., robots) must **adapt continually to new tasks** without forgetting previously learned behaviors.  
This repo provides hands-on implementations of CRL methods and experiments to better understand:

- **Catastrophic forgetting** in RL  
- **Replay-based strategies**  
- **Regularization-based strategies (e.g., EWC, SI)**  
- **Latent representations for generalization across tasks**  

---

## ðŸ§© Implemented Experiments
- [x] Baseline: DQN/Policy Gradient on sequential CartPole â†’ MountainCar  
- [ ] Experience Replay buffer across tasks  
- [ ] Elastic Weight Consolidation (EWC) for CRL  
- [ ] Latent-space representations for transfer across tasks  

---

## ðŸ“‚ Repo Structure
```bash
continual-rl-experiments/
â”œâ”€â”€ agents
â”‚   â”œâ”€â”€ continual_dqn.py
â”‚   â””â”€â”€ dqn.py
â”œâ”€â”€ docs
â”œâ”€â”€ experiments
â”‚   â”œâ”€â”€ run_acrobot.py
â”‚   â”œâ”€â”€ run_all.py
â”‚   â”œâ”€â”€ run_cartpole.py
â”‚   â””â”€â”€ run_mountaincar.py
â”œâ”€â”€ models
â”‚   â”œâ”€â”€ latent.py
â”‚   â””â”€â”€ networks.py
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ utils
    â”œâ”€â”€ evaluation.py
    â””â”€â”€ replay_buffer.py
```

---
## ðŸš€ Usage
```bash
# Clone the repo
git clone https://github.com/cajifan/continual-rl-experiments.git
cd continual-rl-experiments

# Install dependencies
pip install -r requirements.txt


# Run the baseline experiments
python experiments/run_mountaincar.py --method baseline
```
