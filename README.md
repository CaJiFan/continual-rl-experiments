# Continual Reinforcement Learning Experiments

This repository contains a collection of **toy experiments and implementations in Continual Reinforcement Learning (CRL)**.  
The goal is to explore methods that enable RL agents to learn across multiple tasks sequentially while mitigating **catastrophic forgetting** and improving **generalization**.

---

## ðŸŽ¯ Motivation
Traditional RL assumes training on a single task/environment until convergence.  
However, real-world agents (e.g., robots) must **adapt continually to new tasks** without forgetting previously learned behaviors.  
This repo provides hands-on implementations of CRL methods and experiments to better understand:

- **Catastrophic forgetting** in RL  
- **Replay-based strategies**  
- **Regularization-based strategies (e.g., EWC, SI)**  
- **Latent representations for generalization across tasks**  

---

## ðŸ§© Implemented Experiments
- [x] Baseline: DQN/Policy Gradient on sequential CartPole â†’ MountainCar  
- [ ] Experience Replay buffer across tasks  
- [ ] Elastic Weight Consolidation (EWC) for CRL  
- [ ] Latent-space representations for transfer across tasks  

---

## ðŸ“‚ Repo Structure
```bash
continual-rl-experiments/
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ cartpole_mountaincar.py   # Baseline: sequential training (DQN)
â”‚   â””â”€â”€ ewc.py                    # Elastic Weight Consolidation (WIP)
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ example_plot.png          # Example learning curve
â”‚
â”œâ”€â”€ README.md                     # Project overview
â”œâ”€â”€ requirements.txt              # Dependencies
â””â”€â”€ .gitignore
```

---
## ðŸš€ Usage
```bash
# Clone the repo
git clone https://github.com/cajifan/continual-rl-experiments.git
cd continual-rl-experiments

# Install dependencies
pip install -r requirements.txt


# Run the baseline experiments
python experiments/cartpole_mountaincar.py --method baseline
```
